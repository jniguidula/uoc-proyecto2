{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5501bb40-2af1-45ac-a1f2-581a7189a264",
   "metadata": {},
   "source": [
    "## Requisitos 10-12: \n",
    "\n",
    "10. **Modelos de Clasificación**: Desarrollar y optimizar modelos de clasificación (como árboles de decisión, SVM, k-NN), utilizando los métodos adecuados de validación y evaluación.\n",
    "\n",
    "11. **Validación de Modelos**: Seleccionar los mejores modelos mediante validación cruzada con k-fold, para asegurar la robustez y generalización de los modelos creados.\n",
    "    \n",
    "12. **Uso de Scraping para Variables Exógenas**: El proyecto debe incluir el uso de técnicas de web scraping para obtener variables adicionales de fuentes externas que aporten valor a los datos originales del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e65e28b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "#cargar datos\n",
    "current_path = os.getcwd()\n",
    "aguacate_index = current_path.find(\"uoc-proyecto3\")\n",
    "if aguacate_index != -1:\n",
    "        project_root = current_path[:aguacate_index + len(\"uoc-proyecto3\")]\n",
    "else:\n",
    "    raise FileNotFoundError(\"The directory 'uoc-proyecto3' was not found in the path.\")\n",
    "data_path = os.path.join(project_root, 'data', 'Avocado_HassAvocadoBoard_20152023v1.0.1.csv')\n",
    "dataset_avocado_original_df = pd.read_csv(data_path)\n",
    "\n",
    "region_classification = {\n",
    "    'Albany': 'City',\n",
    "    'Atlanta': 'City',\n",
    "    'BaltimoreWashington': 'Region',\n",
    "    'BirminghamMontgomery': 'Region',\n",
    "    'Boise': 'City',\n",
    "    'Boston': 'City',\n",
    "    'BuffaloRochester': 'Region',\n",
    "    'California': 'GreaterRegion',\n",
    "    'Charlotte': 'City',\n",
    "    'Chicago': 'City',\n",
    "    'CincinnatiDayton': 'Region',\n",
    "    'Columbus': 'City',\n",
    "    'DallasFtWorth': 'Region',\n",
    "    'Denver': 'City',\n",
    "    'Detroit': 'City',\n",
    "    'GrandRapids': 'City',\n",
    "    'GreatLakes': 'GreaterRegion',\n",
    "    'HarrisburgScranton': 'Region',\n",
    "    'HartfordSpringfield': 'Region',\n",
    "    'Houston': 'City',\n",
    "    'Indianapolis': 'City',\n",
    "    'Jacksonville': 'City',\n",
    "    'LasVegas': 'City',\n",
    "    'LosAngeles': 'City',\n",
    "    'Louisville': 'City',\n",
    "    'Miami': 'City',\n",
    "    'MiamiFtLauderdale': 'Region',\n",
    "    'Midsouth': 'GreaterRegion',\n",
    "    'Nashville': 'City',\n",
    "    'NewOrleans': 'City',\n",
    "    'NewYork': 'City',\n",
    "    'Northeast': 'GreaterRegion',\n",
    "    'NorthernNewEngland': 'Region',\n",
    "    'Orlando': 'City',\n",
    "    'PeoriaSpringfield': 'Region',\n",
    "    'Philadelphia': 'City',\n",
    "    'PhoenixTucson': 'Region',\n",
    "    'Pittsburgh': 'City',\n",
    "    'Plains': 'GreaterRegion',\n",
    "    'Portland': 'City',\n",
    "    'Providence': 'City',\n",
    "    'RaleighGreensboro': 'Region',\n",
    "    'RichmondNorfolk': 'Region',\n",
    "    'Roanoke': 'City',\n",
    "    'Sacramento': 'City',\n",
    "    'SanDiego': 'City',\n",
    "    'SanFrancisco': 'City',\n",
    "    'Seattle': 'City',\n",
    "    'SouthCarolina': 'Region',\n",
    "    'SouthCentral': 'GreaterRegion',\n",
    "    'Southeast': 'GreaterRegion',\n",
    "    'Spokane': 'City',\n",
    "    'StLouis': 'City',\n",
    "    'Syracuse': 'City',\n",
    "    'Tampa': 'City',\n",
    "    'Toledo': 'City',\n",
    "    'TotalUS': 'TotalUS',\n",
    "    'West': 'GreaterRegion',\n",
    "    'WestTexNewMexico': 'Region',\n",
    "    'Wichita': 'City'\n",
    "}\n",
    "\n",
    "def map_regions(original_data: pd.DataFrame, region_map: dict, guardar: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Asigna la clasificación de regiones y ciudades al dataframe original de aguacate\n",
    "    en una nueva columna region_type\n",
    "\n",
    "    Parametros:\n",
    "    - original_data: pd.DataFrame-  Datos originales avocado.csv\n",
    "    - region_map: Dict[str, str] - El mapping de agrupaciones\n",
    "    - guardar: Boolean - True or False para guardar nuevo csv o no\n",
    "\n",
    "    Regresa:\n",
    "    - pd.DataFrame: Dataframe actualizado de los datos originales\n",
    "    \"\"\"\n",
    "\n",
    "    path_salida = \"data/avocado_with_region_types.csv\"\n",
    "    nuevo_aguacate_df = original_data.copy()\n",
    "    nuevo_aguacate_df['region_type'] = nuevo_aguacate_df['region'].map(region_map)\n",
    "    if guardar:\n",
    "        print(f\"Guardando archivo .csv en /data/  ...\")\n",
    "        nuevo_aguacate_df.to_csv(path_salida, index=False)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return nuevo_aguacate_df\n",
    "\n",
    "def obtener_nuevo_avocado()-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Función que devuelve dataframe con columna region_type\n",
    "\n",
    "    Regresa:\n",
    "    - pd.DataFrame: DataFrame con columna nueva que agrupa regiones para análisis\n",
    "    \"\"\"\n",
    "\n",
    "    nuevo_avocado_df = map_regions(dataset_avocado_original_df, region_classification, guardar=False)\n",
    "    return nuevo_avocado_df\n",
    "\n",
    "def imputar_fechas()-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Función que imouta por promedio las tres entradas faltantes en el avocado.csv original\n",
    "\n",
    "    regresa:\n",
    "    - pd.DataFrame: con las tres fechas para el type organic en WestTexNewMexico\n",
    "    \"\"\"\n",
    "\n",
    "    df = obtener_nuevo_avocado()\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    # Fechas y parámetros específicos para imputación\n",
    "    missing_dates = ['2015-12-06', '2017-06-18', '2017-06-25']\n",
    "    region = 'WestTexNewMexico'\n",
    "    avocado_type = 'organic'\n",
    "\n",
    "    # Iterar sobre las fechas faltantes para imputar valores\n",
    "    for date in missing_dates:\n",
    "        # Convertir la fecha a datetime\n",
    "        date = pd.to_datetime(date)\n",
    "\n",
    "        # Filtrar las filas previas y posteriores a la fecha faltante\n",
    "        prev_row = df[(df['Date'] < date) &\n",
    "                      (df['region'] == region) &\n",
    "                      (df['type'] == avocado_type)].sort_values(by='Date').iloc[-1]\n",
    "        next_row = df[(df['Date'] > date) &\n",
    "                      (df['region'] == region) &\n",
    "                      (df['type'] == avocado_type)].sort_values(by='Date').iloc[0]\n",
    "\n",
    "        # Calcular el promedio de los valores numéricos entre las dos fechas\n",
    "        imputed_values = prev_row.copy()\n",
    "        for col in df.select_dtypes(include='number').columns:\n",
    "            imputed_values[col] = (prev_row[col] + next_row[col]) / 2\n",
    "\n",
    "        # Asignar la fecha, región y tipo específico a la fila imputada\n",
    "        imputed_values['Date'] = date\n",
    "        imputed_values['region'] = region\n",
    "        imputed_values['type'] = avocado_type\n",
    "\n",
    "        # Añadir la fila imputada al DataFrame\n",
    "        df = pd.concat([df, pd.DataFrame([imputed_values])], ignore_index=True)\n",
    "\n",
    "        # Ordenar el DataFrame por fecha para mantener el orden cronológico\n",
    "        df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fdce10-5c3f-4e7f-a4e3-7ba51c072c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar librerias\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_orig = obtener_nuevo_avocado()\n",
    "\n",
    "#df_orig = pd.read_csv(\"avocado_with_region_types.csv\")\n",
    "\n",
    "# guardamos el original y trabajamos con una copia\n",
    "df = df_orig.copy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02e2c9a4-f508-42d1-a5ad-e993533a1c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid date 2015-2-29: day is out of range for month\n",
      "Skipping invalid date 2015-2-30: day is out of range for month\n",
      "Skipping invalid date 2015-2-31: day is out of range for month\n",
      "Skipping invalid date 2015-4-31: day is out of range for month\n",
      "Skipping invalid date 2015-6-31: day is out of range for month\n",
      "Skipping invalid date 2015-9-31: day is out of range for month\n",
      "Skipping invalid date 2015-11-31: day is out of range for month\n",
      "Skipping invalid date 2016-2-30: day is out of range for month\n",
      "Skipping invalid date 2016-2-31: day is out of range for month\n",
      "Skipping invalid date 2016-4-31: day is out of range for month\n",
      "Skipping invalid date 2016-6-31: day is out of range for month\n",
      "Skipping invalid date 2016-9-31: day is out of range for month\n",
      "Skipping invalid date 2016-11-31: day is out of range for month\n",
      "Skipping invalid date 2017-2-29: day is out of range for month\n",
      "Skipping invalid date 2017-2-30: day is out of range for month\n",
      "Skipping invalid date 2017-2-31: day is out of range for month\n",
      "Skipping invalid date 2017-4-31: day is out of range for month\n",
      "Skipping invalid date 2017-6-31: day is out of range for month\n",
      "Skipping invalid date 2017-9-31: day is out of range for month\n",
      "Skipping invalid date 2017-11-31: day is out of range for month\n",
      "Skipping invalid date 2018-2-29: day is out of range for month\n",
      "Skipping invalid date 2018-2-30: day is out of range for month\n",
      "Skipping invalid date 2018-2-31: day is out of range for month\n",
      "Skipping invalid date 2018-4-31: day is out of range for month\n",
      "Skipping invalid date 2018-6-31: day is out of range for month\n",
      "Skipping invalid date 2018-9-31: day is out of range for month\n",
      "Skipping invalid date 2018-11-31: day is out of range for month\n",
      "Skipping invalid date 2019-2-29: day is out of range for month\n",
      "Skipping invalid date 2019-2-30: day is out of range for month\n",
      "Skipping invalid date 2019-2-31: day is out of range for month\n",
      "Skipping invalid date 2019-4-31: day is out of range for month\n",
      "Skipping invalid date 2019-6-31: day is out of range for month\n",
      "Skipping invalid date 2019-9-31: day is out of range for month\n",
      "Skipping invalid date 2019-11-31: day is out of range for month\n",
      "Skipping invalid date 2020-2-30: day is out of range for month\n",
      "Skipping invalid date 2020-2-31: day is out of range for month\n",
      "Skipping invalid date 2020-4-31: day is out of range for month\n",
      "Skipping invalid date 2020-6-31: day is out of range for month\n",
      "Skipping invalid date 2020-9-31: day is out of range for month\n",
      "Skipping invalid date 2020-11-31: day is out of range for month\n",
      "Skipping invalid date 2021-2-29: day is out of range for month\n",
      "Skipping invalid date 2021-2-30: day is out of range for month\n",
      "Skipping invalid date 2021-2-31: day is out of range for month\n",
      "Skipping invalid date 2021-4-31: day is out of range for month\n",
      "Skipping invalid date 2021-6-31: day is out of range for month\n",
      "Skipping invalid date 2021-9-31: day is out of range for month\n",
      "Skipping invalid date 2021-11-31: day is out of range for month\n",
      "Skipping invalid date 2022-2-29: day is out of range for month\n",
      "Skipping invalid date 2022-2-30: day is out of range for month\n",
      "Skipping invalid date 2022-2-31: day is out of range for month\n",
      "Skipping invalid date 2022-4-31: day is out of range for month\n",
      "Skipping invalid date 2022-6-31: day is out of range for month\n",
      "Skipping invalid date 2022-9-31: day is out of range for month\n",
      "Skipping invalid date 2022-11-31: day is out of range for month\n",
      "Skipping invalid date 2023-2-29: day is out of range for month\n",
      "Skipping invalid date 2023-2-30: day is out of range for month\n",
      "Skipping invalid date 2023-2-31: day is out of range for month\n",
      "Skipping invalid date 2023-4-31: day is out of range for month\n",
      "Skipping invalid date 2023-6-31: day is out of range for month\n",
      "Skipping invalid date 2023-9-31: day is out of range for month\n",
      "Skipping invalid date 2023-11-31: day is out of range for month\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the API and your API key\n",
    "api_url = \"http://api.openweathermap.org/data/2.5/onecall/timemachine\"\n",
    "api_key = \"d3679ce88231385b5b6b479378f87095\"\n",
    "\n",
    "# Latitude and Longitude for a region like California\n",
    "lat = 36.7783\n",
    "lon = -119.4179\n",
    "\n",
    "# Example for a range of years: 2015 to 2023\n",
    "years = range(2015, 2024)\n",
    "\n",
    "# Placeholder for weather data\n",
    "weather_data = []\n",
    "\n",
    "for year in years:\n",
    "    for month in range(1, 13):  # Loop through months 1-12\n",
    "        for day in range(1, 32):  # Loop through days (we'll filter out invalid dates later)\n",
    "            try:\n",
    "                # Create a date object\n",
    "                date = datetime(year, month, day)\n",
    "                timestamp = int(date.timestamp())\n",
    "                \n",
    "                # Make the request\n",
    "                params = {\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon,\n",
    "                    \"dt\": timestamp,\n",
    "                    \"appid\": api_key\n",
    "                }\n",
    "                response = requests.get(api_url, params=params)\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    weather_data.append({\n",
    "                        \"date\": date,\n",
    "                        \"temperature\": data['current']['temp'],\n",
    "                        \"humidity\": data['current']['humidity'],\n",
    "                        \"rain\": data['current'].get('rain', {}).get('1h', 0),\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Skipping invalid date {year}-{month}-{day}: {e}\")\n",
    "\n",
    "# Convert the collected weather data into a DataFrame\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Display the weather data for the first few rows\n",
    "print(weather_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15631d9e-da5a-4671-9eae-ad63fdbfd2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch data. Status code: 401\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "# Replace with your API key and coordinates\n",
    "api_key = \"2e5a5a74e6b1270a7c7233dbea1f67ec\"\n",
    "lat = 36.7783  # Example: latitude for California\n",
    "lon = -119.4179  # Example: longitude for California\n",
    "\n",
    "# Request weather data for a specific day (timestamp for testing)\n",
    "timestamp = int(datetime(2020, 1, 1).timestamp())  # Example date: January 1, 2020\n",
    "\n",
    "# API URL\n",
    "api_url = \"http://api.openweathermap.org/data/2.5/onecall/timemachine\"\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    \"lat\": lat,\n",
    "    \"lon\": lon,\n",
    "    \"dt\": timestamp,\n",
    "    \"appid\": api_key\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "response = requests.get(api_url, params=params)\n",
    "\n",
    "# Check if the response is successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    print(\"API Response:\", data)  # Check the contents of the response\n",
    "else:\n",
    "    print(f\"Failed to fetch data. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc873b5d-7f9b-4e6a-b526-068295fc1287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data has been saved to data/weather_data.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'weather_df' is your weather DataFrame\n",
    "\n",
    "# Specify the file path where you want to save the CSV file\n",
    "file_path = 'data/weather_data.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "weather_df.to_csv(file_path, index=False)\n",
    "\n",
    "# Confirmation message\n",
    "print(f\"Weather data has been saved to {file_path}\")\n",
    "weather_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f78311d9-f149-42f9-9cb5-e564b61156ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "TooManyRequestsError",
     "evalue": "The request failed: Google returned a response with code 429",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTooManyRequestsError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Add a delay of 5 seconds between requests\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Retrieve the data\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m interest_over_time_df \u001b[38;5;241m=\u001b[39m \u001b[43mpytrends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterest_over_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Save the data to a CSV file\u001b[39;00m\n\u001b[1;32m     23\u001b[0m interest_over_time_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavocado_trends.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytrends/request.py:232\u001b[0m, in \u001b[0;36mTrendReq.interest_over_time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    224\u001b[0m over_time_payload \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# convert to string as requests will mangle\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreq\u001b[39m\u001b[38;5;124m'\u001b[39m: json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterest_over_time_widget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtz\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtz\n\u001b[1;32m    229\u001b[0m }\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# make the request and parse the returned json\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m req_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINTEREST_OVER_TIME_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTrendReq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET_METHOD\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrim_chars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mover_time_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(req_json[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimelineData\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (df\u001b[38;5;241m.\u001b[39mempty):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pytrends/request.py:159\u001b[0m, in \u001b[0;36mTrendReq._get_data\u001b[0;34m(self, url, method, trim_chars, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m status_codes\u001b[38;5;241m.\u001b[39mcodes\u001b[38;5;241m.\u001b[39mtoo_many_requests:\n\u001b[0;32m--> 159\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTooManyRequestsError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mResponseError\u001b[38;5;241m.\u001b[39mfrom_response(response)\n",
      "\u001b[0;31mTooManyRequestsError\u001b[0m: The request failed: Google returned a response with code 429"
     ]
    }
   ],
   "source": [
    "#import os\n",
    "#!pip3 install pytrends\n",
    "\n",
    "import time\n",
    "from pytrends.request import TrendReq\n",
    "\n",
    "# Initialize pytrends object\n",
    "pytrends = TrendReq(hl='en-US', tz=360)\n",
    "\n",
    "# Define keywords\n",
    "keywords = ['avocados']\n",
    "\n",
    "# Get interest over time data with time delays\n",
    "pytrends.build_payload(keywords, cat=0, timeframe='2015-01-01 2023-12-31', geo='US', gprop='')\n",
    "\n",
    "# Use a delay between requests to avoid hitting the rate limit\n",
    "time.sleep(5)  # Add a delay of 5 seconds between requests\n",
    "\n",
    "# Retrieve the data\n",
    "interest_over_time_df = pytrends.interest_over_time()\n",
    "\n",
    "# Save the data to a CSV file\n",
    "interest_over_time_df.to_csv('avocado_trends.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "753b7a1a-52c8-48c7-a91b-1d73ce25ed90",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5199/2413455170.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#df_avocados = pd.read_csv(\"avocados_kaggle.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_avocados\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Merge weather data (based on date or region)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf_enriched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_avocados\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweather_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Now df_enriched contains avocado data along with exogenous variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright_join_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m             \u001b[0mleft_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mright_drop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         ) = self._get_merge_keys()\n\u001b[0m\u001b[1;32m    795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mleft_drop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_labels_or_levels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                         \u001b[0;31m# Then we're either Hashable or a wrong-length arraylike,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                         \u001b[0;31m#  the latter of which will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                         \u001b[0mrk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHashable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mrk\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m                             \u001b[0;31m# work-around for merge_asof(right_index=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1300\u001b[0m                             \u001b[0mright_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1908\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1911\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1913\u001b[0m         \u001b[0;31m# Check for duplicates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'date'"
     ]
    }
   ],
   "source": [
    "# Assuming you have loaded your avocado dataset into df2\n",
    "#df_avocados = pd.read_csv(\"avocados_kaggle.csv\")\n",
    "df_avocados=df.copy()\n",
    "\n",
    "# Merge weather data (based on date or region)\n",
    "df_enriched = pd.merge(df_avocados, weather_df, left_on=\"Date\", right_on=\"date\", how=\"left\")\n",
    "\n",
    "\n",
    "# Now df_enriched contains avocado data along with exogenous variables\n",
    "print(df_enriched.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2dfd81-8f0e-4a3d-95b1-3770c0b0e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_enriched is the dataset with avocado data and weather data\n",
    "\n",
    "# Plotting price vs. temperature, humidity, and rain\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Price vs Temperature\n",
    "sns.scatterplot(x=df_enriched['temperature'], y=df_enriched['AveragePrice'], ax=axes[0])\n",
    "axes[0].set_title('Price vs Temperature')\n",
    "\n",
    "# Price vs Humidity\n",
    "sns.scatterplot(x=df_enriched['humidity'], y=df_enriched['AveragePrice'], ax=axes[1])\n",
    "axes[1].set_title('Price vs Humidity')\n",
    "\n",
    "# Price vs Rain\n",
    "sns.scatterplot(x=df_enriched['rain'], y=df_enriched['AveragePrice'], ax=axes[2])\n",
    "axes[2].set_title('Price vs Rain')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plotting volume vs. weather variables\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Volume vs Temperature\n",
    "sns.scatterplot(x=df_enriched['temperature'], y=df_enriched['TotalVolume'], ax=axes[0])\n",
    "axes[0].set_title('Volume vs Temperature')\n",
    "\n",
    "# Volume vs Humidity\n",
    "sns.scatterplot(x=df_enriched['humidity'], y=df_enriched['TotalVolume'], ax=axes[1])\n",
    "axes[1].set_title('Volume vs Humidity')\n",
    "\n",
    "# Volume vs Rain\n",
    "sns.scatterplot(x=df_enriched['rain'], y=df_enriched['TotalVolume'], ax=axes[2])\n",
    "axes[2].set_title('Volume vs Rain')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
